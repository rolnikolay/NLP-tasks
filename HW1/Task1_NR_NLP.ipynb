{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Homework1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1PLujMj7f7S",
        "colab_type": "text"
      },
      "source": [
        "## Homework 1\n",
        "### NLP Basics & NLP Pipelines\n",
        "\n",
        "Welcome to Homework 1! \n",
        "\n",
        "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _six_.\n",
        "\n",
        "The **grading** for each task is the following:\n",
        "- correct answer - **full points**\n",
        "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
        "- no answer or completely wrong solution - **no points**\n",
        "\n",
        "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
        "\n",
        "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
        "\n",
        "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
        "\n",
        "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
        "\n",
        "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64e7Qq2N7f7W",
        "colab_type": "code",
        "outputId": "39dde83c-a4bd-4bcd-b998-c3ebd13108ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict, Counter"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5kMc2AD7f7c",
        "colab_type": "text"
      },
      "source": [
        "### Task 1. Find the data (0.5 points)\n",
        "\n",
        "Find large enough text data in English or [any other language supported by Spacy](https://spacy.io/usage/models). If the resources for your language are very limited, you may use English or other language of your preference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7qZ2wS_7f7h",
        "colab_type": "text"
      },
      "source": [
        "**What is the language of your data?**\n",
        "\n",
        "<font color='green'>English</font>\n",
        "\n",
        "**Where did you get the text data?**\n",
        "\n",
        "<font color='green'>[Project Gutenberg](https://www.gutenberg.org/ebooks/1998)</font>\n",
        "\n",
        "**What kind of text is it? (books, magazines, news articles, etc.)**\n",
        "\n",
        "<font color='green'>Text from a book [\"Thus Spake Zarathustra by\"](https://www.gutenberg.org/files/1998/1998-0.txt)</font>\n",
        "\n",
        "**What style(s) of text does your data have? (user commetaries, scientific, neutral, etc.)**\n",
        "\n",
        "<font color='green'>Philosophy, Liturature. I assume there might be complications as it uses non-conventional language to some extend.</font>\n",
        "\n",
        "**Was it easy to download the data? If no, desribe what difficulties you had and how you resolved them.**\n",
        "\n",
        "<font color='green'>It was fine. I clicked \"Ctrl+S\" on [this page](https://www.gutenberg.org/files/1998/1998-0.txt) and it saved the file on my Desktop.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_AQ8qQw7f7k",
        "colab_type": "text"
      },
      "source": [
        "### Task 2. Tokenize and count statistics (0.5 points)\n",
        "\n",
        "Using either NLTK or Spacy tools, tokenize your text data that you found in the previous exercise.\n",
        "\n",
        "P.S. if you are using Spacy, don't forget to load an appropriate module for it\n",
        "\n",
        "Compute and output the following:\n",
        "- number of sentences \n",
        "- number of tokens \n",
        "- number of unique tokens (or types)\n",
        "- average length of a sentence\n",
        "- average length of a token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILLRr9JV7f7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace the path with the name of your data file\n",
        "data_path = \"text.txt\"\n",
        "\n",
        "data = open(data_path, encoding='utf-8').read()\n",
        "data\n",
        "# Split the data into sentences and tokens\n",
        "print('Using NLTK:')\n",
        "print(word_tokenize(data))\n",
        "print(sent_tokenize(data))\n",
        "print('------------------')\n",
        "print('------------------')\n",
        "\n",
        "\n",
        "print('Using Spacy:')\n",
        "doc = nlp(data)\n",
        "print([token.text for token in doc])\n",
        "print([sents for sents in doc.sents])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpZOv5gU7f7r",
        "colab_type": "code",
        "outputId": "b5f367f0-c0da-40d4-dc89-a3f251051d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "num_sentences = len(list(doc.sents))\n",
        "num_tokens = len([token.text for token in doc])\n",
        "num_unique_tokens = len(set([token.text for token in doc]))\n",
        "avg_sentence_len = round(num_tokens/num_sentences,2)\n",
        "avg_token_len = round(sum([len(token.text) for token in doc])/len([token.text for token in doc]),2)\n",
        "\n",
        "print(\"Number of sentences:\", num_sentences)\n",
        "print(\"Number of tokens:\", num_tokens)\n",
        "print(\"Number of unique tokens (or types):\", num_unique_tokens)\n",
        "print(\"Average sentence length:\", avg_sentence_len)\n",
        "print(\"Average token length:\", avg_token_len)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: 9716\n",
            "Number of tokens: 151095\n",
            "Number of unique tokens (or types): 12209\n",
            "Average sentence length: 15.55\n",
            "Average token length: 3.68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHX-qiuI7f7z",
        "colab_type": "text"
      },
      "source": [
        "### Task 3. Byte pair encoding (BPE) tokenization (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C_PmQX-7f71",
        "colab_type": "text"
      },
      "source": [
        "#### Task 3.1 (0.25 points)\n",
        "\n",
        "[Byte pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) is a simple algorithm of data compression. It looks for the most frequent pair of bytes in the data and replaces it with a new byte which is not seen in the data. \n",
        "\n",
        "Recently, this idea became [used in the tokenization](https://www.aclweb.org/anthology/P16-1162.pdf). Let's say that we want to train a network that captures the meaning of words. We can have in out data the following words: `low`, `lower`, `lowest`. If we tokenize the text in a simple way by splitting the words as a whole, the model will probably learn the relation between `low`, `lower`, `lowest`. Now, imagine that we get some new text that the model didn't see during training and it has the words `small`, `smaller`, `smallest` and in the training data we had only the word `small`. Since the model didn't see `smaller` and `smallest` during the training, it will most likely fail to capture the relation.\n",
        "\n",
        "One of the ways to solve this is BPE tokenization. It learns the most frequent sequences and can split an unknown word into **subwords**. In our case, it can split `smaller` into `['small', 'er']` since we had `small` in the training data and probably many other words ending with -er. Now. instead of one unknown word, the model have two known subwords from which it can take the information.\n",
        "\n",
        "The code below builds the subwords from the text data. For the purpose of time saving, we set the number of merges to 1000. \n",
        "\n",
        "Study the code below and answer the questions after it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sec0p3h97f73",
        "colab_type": "code",
        "outputId": "a0148335-d25c-45db-fe49-79e5bbef62d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "filename = data\n",
        "def get_vocab(filename):\n",
        "    \"\"\"Gets the text from a file and splits it with spaces.\"\"\"\n",
        "    \n",
        "    vocab = Counter()\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    \"\"\"Computes the frequencies for each pair of characters in the vocab.\"\"\"\n",
        "\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, in_vocab):\n",
        "    \"\"\"Merges the most frequent pair.\n",
        "\n",
        "    Arguments:\n",
        "    pair -- the most frequent word pair (tuple(str, str))\n",
        "    in_vocab -- vocabulary with frequencies (dict)\n",
        "    \"\"\"\n",
        "    \n",
        "    out_vocab = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in in_vocab:\n",
        "        out_word = p.sub(''.join(pair), word)\n",
        "        out_vocab[out_word] = in_vocab[word]\n",
        "    return out_vocab\n",
        "\n",
        "def get_tokens_from_vocab(vocab):\n",
        "    tokens_frequencies = Counter()\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "def measure_token_length(token):\n",
        "    if token[-4:] == '</w>':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)\n",
        "\n",
        "vocab = get_vocab(data_path)\n",
        "\n",
        "print('\\n==========')\n",
        "print('Tokens Before BPE')\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "print('==========')\n",
        "\n",
        "num_merges = 1000\n",
        "for i in tqdm(range(num_merges)):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "\n",
        "print('\\nAll tokens: {}'.format(tokens_frequencies.keys()))\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "print('==========')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1000 [00:00<01:55,  8.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "==========\n",
            "Tokens Before BPE\n",
            "All tokens: dict_keys(['\\ufeff', 'T', 'h', 'e', '</w>', 'P', 'r', 'o', 'j', 'c', 't', 'G', 'u', 'n', 'b', 'g', 'E', 'B', 'k', 'f', 's', 'S', 'p', 'a', 'Z', ',', 'y', 'F', 'i', 'd', 'N', 'z', 'w', 'l', 'm', 'v', '.', 'Y', '-', 'L', ':', 'A', 'C', 'D', '7', '2', '0', '8', '[', '#', '1', '9', ']', 'R', 'U', '6', '*', 'O', 'H', 'I', 'J', 'K', '’', 'q', 'W', 'M', 'V', 'X', '“', '”', ';', 'x', '‘', '3', '5', '?', '(', ')', '!', 'Q', '4', '_', '/', '%', '@', '$'])\n",
            "Number of tokens: 86\n",
            "==========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:36<00:00, 10.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "All tokens: dict_keys(['\\ufeff', 'The</w>', 'Project</w>', 'Gutenber', 'g</w>', 'E', 'B', 'ook</w>', 'of</w>', 'Thus</w>', 'S', 'p', 'ake</w>', 'Zarathustra,</w>', 'by</w>', 'F', 'ri', 'ed', 'ch</w>', 'Nietzsche</w>', 'This</w>', 'e', 'is</w>', 'for</w>', 'the</w>', 'use</w>', 'an', 'y', 'one</w>', 'where</w>', 'at</w>', 'no</w>', 'co', 'st</w>', 'and</w>', 'with</w>', 'al', 'most</w>', 'r', 'est', 'c', 'tion', 's</w>', 'w', 'hat', 'so', 'ever', '.</w>', 'Y', 'ou</w>', 'may</w>', 'op', 'y</w>', 'it,</w>', 'give</w>', 'it</w>', 'away</w>', 'or</w>', 're', '-', 'un', 'der</w>', 'ter', 'm', 'L', 'ic', 'en', 'se</w>', 'in', 'cl', 'ud', 'ed</w>', 'this</w>', 'on', 'l', 'ine</w>', '.', 'g', 'utenber', 'or', 'T', 'it', 'le', ':</w>', 'Zarathustra</w>', 'A</w>', 'All</w>', 'N', 'A', 'u', 'th', 's', 'at', 'Th', 'om', 'as</w>', 'C', 'on</w>', 'P', 'o', 'st', 'ing</w>', 'D', 'e:</w>', 'ov', 'em', 'b', 'er</w>', '7', ',</w>', '2', '0', '8', '</w>', '[', '#', '1', '9', ']', 'R', 'a', 'ce', 'er,</w>', 'U', 'd', 'ed:</w>', '6', 'ang', 'ag', 'n', 'li', 'sh', 'har', 'ac', 'ter</w>', 'se', 't</w>', 'enc', 'od', 'ing', '*', 'ST', 'AR', 'T</w>', 'O', 'TH', 'I', 'S</w>', 'J', 'G', 'EN', 'ER', 'K', 'E</w>', 'Z', 'Pro', 'e</w>', 'sch', 'OR', 'AN', 'D</w>', 'ON', 'ated</w>', '’s</w>', 'No', 't', 'ha', 'ic</w>', 'sp', 'ell', 'tu', 'ation</w>', 'sa', 'es</w>', 'have</w>', 'not</w>', 'been</w>', 'ch', 'ed.</w>', 'I</w>', 'par', 'ul', 'ar</w>', 'qu', 'ation', 'are</w>', 'of', 'ten</w>', 'lo', 'sed</w>', 'ver', 'al</w>', 'ra', 'ph', 's.</w>', 'W', 'IN', 'Y</w>', 'M', 'H', 'Zarathustra', 'ol', 'gu', 'e.</w>', 'is', 'our', 'es.</w>', 'I.</w>', 'ree</w>', 'et', 'am', 'II', 'ca', 'de', 'ir', 'V', 'irtu', 'k', 'world', 'sm', 'es', 'ers</w>', 'y.</w>', 'as', 'si', 'im', 'VII', 'ad', 'ing.</w>', 'ill', 'X', 'ach', 'ath', 'ar', 'w</w>', 'in</w>', 'pla', 'end', 'ous', 'On', 'Go', 'igh', 'ay</w>', 'd</w>', 'oun', 'ite</w>', 'der', 'XX', 'h', 'il', 'ary</w>', 'ow', 'SE', 'In</w>', 'app', 'iti', 'ful', 'ou', 'ab', 'ble', 'la', 'ous</w>', 'ise</w>', 'ong', 'ance', 've', 'el', 'f', 'ur', 'pas', 'me</w>', 'ture', 'ate</w>', 'er', 'cep', 'v', 'ent', 'oo', 'say', 'er.</w>', 'mp', 'ly</w>', 'ru', 'ence', 'est</w>', 'i', 'ig', 'ma', 'vol', 'fore</w>', 'war', 'n</w>', 'ount', 'ing-', 'po', 'il</w>', 'val', 'con', 'even</w>', 'eal', 'OU', 'one', 'str', 'ess.</w>', 'k</w>', 'ici', 'ut</w>', 'gg', 'ide', 'Sup', 'per', 'ong</w>', 'anch', 'ci', 'ugh', 't.</w>', 'wa', 'ti', 'run', 'ken</w>', 'gn', '“', '”</w>', 'do', 'O</w>', 'my</w>', 'bro', 'ther', 'work', ';</w>', 'ist', 'his</w>', 'div', 'id', 'ex', 'es,</w>', 'friend', 's,</w>', 'tur', 'dis', 'ap', 'int', 'ment', 'bo', 've</w>', 'all', 'however,</w>', 'there</w>', 'tr', 'fi', 'age</w>', 'greatest</w>', 'hop', 'mo', 'My</w>', 'ther</w>', 'had</w>', 'ure</w>', 'ind</w>', 'from</w>', 'very</w>', 'ear', 'you', 'he</w>', 'once</w>', 'old</w>', 'that</w>', 'a</w>', 'chil', 'dre', 'him', 'At</w>', 'di', 'ffer', 'ent</w>', 'lif', 'e,</w>', 'would</w>', 'call</w>', 'nam', 'but</w>', ',”</w>', 'no', 'te</w>', 'sub', 'j', 'ec', 't,</w>', '“I</w>', 'to</w>', 'do</w>', 'hon', 'our</w>', 'enti', 'ying</w>', 'him</w>', 'creat', 'anc', 'were</w>', 'first</w>', 'take</w>', 'ad</w>', 'com', 'pre', 'vi', 'ser', 'ev', 'acc', 'them,</w>', 'was</w>', 'pres', 'over</w>', 'pro', 'he', 'every</w>', '‘', 'Ha', 'z', ',', '’', '--', 'ast', '.”</w>', 'ew', 'also</w>', 'ity,</w>', 'd.</w>', 'Wh', 'ever</w>', 'read', 'pos', 'um', 'wr', 'ings</w>', 'ears</w>', 'car', 'will</w>', 'ant', 'me', 'et</w>', 'su', 'though', 'ts</w>', 'For</w>', 'Super', 'man</w>', 'put</w>', 'for', 'th</w>', 'lear', 'all</w>', '3', '5', 'gi', '”', 'fol', 'low', 'mar', 'able</w>', 'ob', 'cc', ':--</w>', 'How</w>', 'can</w>', 'pra', 'gl', 'if', 'wh', '?', 'ven</w>', 'among</w>', '“The</w>', 'inter', 'imp', 'ort', 'ant</w>', 'because</w>', 'they</w>', 'such</w>', 'great</w>', '?</w>', 'tion</w>', 'which</w>', 'ough', 'be</w>', 'am</w>', 'only</w>', 'people</w>', 'man,</w>', 'ally</w>', 'av', 'oura', 'ble</w>', 'dev', 'ment</w>', 'any</w>', 'mean', 'goo', 'ness</w>', 'peop', 'le,</w>', 'les</w>', 'their</w>', 'evil</w>', 'THE</w>', 'RE', 'AT</w>', 'ere</w>', 'we</w>', 'still</w>', 'men', 'new</w>', 'm</w>', 'an</w>', 'already</w>', 'th,</w>', 'IT', '(', 'ving</w>', 'ce</w>', 'nothing</w>', 'its</w>', 'ut', ')</w>', 'But</w>', 'those</w>', 'day', 'longer</w>', 'hel', 'highest</w>', 'ty', 'around</w>', 'hu', 'man', '--the</w>', 'be', 'Who</w>', 'ell</w>', 'what</w>', 'That</w>', 'y,</w>', 'after</w>', 'wor', 'bl', '--that</w>', 'light</w>', 'valu', 'cri', 'emp', 'yet</w>', 'hath</w>', 'ak', 'en</w>', 'Al', '-t', 'ea', 'other', 'Ver', 'ily</w>', 'found</w>', '!', '”--</w>', 'mis', 'under', 'se,</w>', 'higher</w>', 'whi', 'ion', 'now</w>', 'le</w>', 'ood</w>', 'cor', 'other</w>', ':--', 'Or', 'ill</w>', 'He</w>', 'res', 'eak', 'bea', 'pr', 'd,</w>', 'pow', 'end</w>', 'life</w>', 'must</w>', 'ind', 'over', 'fl', 'who</w>', 'before</w>', 'us</w>', 'will', 'And</w>', 'just</w>', 'ing,</w>', 'led</w>', 'suffer', 'so</w>', 'ge', 'self', 'bad', 'pe</w>', 'ard', 'ful</w>', 'ne', 'some</w>', 'fin', 'ely</w>', 'and', 'eci', 'sen', 'know</w>', 'there', 'some', 'sur', 'af', 'ity</w>', 'men</w>', 'could</w>', 'spirit', 'ted</w>', 'ight</w>', 'gre', 'through</w>', 'ere', 'well</w>', 'ati', 'tim', 'ould</w>', 'ven', 'mad', 'more</w>', 'tw', 'o</w>', 'ff', 'ect</w>', 'with', 'cal', 'l</w>', 'shall</w>', 'ear</w>', 'fa', 'ith</w>', 'cre', 'e?</w>', 'ject</w>', 'ses</w>', 'ess', 'always</w>', 'y-', 'the', ')', 'well', 'pe', 'above</w>', 'des', 'himself</w>', 'light', 'conc', 'ern', 'ert', 'ain</w>', 'ge</w>', 'stand</w>', 's:</w>', 'how</w>', 'ess</w>', 'than</w>', 'last</w>', 'hap', 'ess,</w>', 'hard', 'to-', '”--', ',--', 'fir', 'fu', 'ire</w>', 'thin', 'mer', 'hi', 'to', 'ose</w>', 'soul</w>', 'long', 'eth</w>', 'ence</w>', 'ure', 'wan', 'fe', 'disc', 'ke', 'wise</w>', 'form', 'thing</w>', 'pur', 'again', 'ice</w>', 'now', 'long</w>', 'way</w>', 'perhaps</w>', 'ent,</w>', 'enough</w>', 'ck', 'br', 'less</w>', 'dang', 'thy</w>', 'seem', 'comp', 'ound', 'en,</w>', 'bey', 'ond</w>', 'own</w>', 'world</w>', 'strang', 'fri', 'gh', 'thir', 'go', 'out</w>', 'hand', '!</w>', '!--</w>', 'cont', 'out', 'loo', 'consci', 'ness', 'should</w>', 'day</w>', 'them', 'like</w>', 'one,</w>', 'know', 'led', 'spirit</w>', 'say</w>', 'ance</w>', 'everything</w>', 'called</w>', 'e;</w>', 'whom</w>', 'x', 'human</w>', 'ple', 'when</w>', 'ide</w>', 'pa', 'mor', 'tru', '--and</w>', 'ro', 'up', 'tra', 'beg', 'though</w>', 'work</w>', 'much</w>', 'did</w>', 'come</w>', 'into</w>', 'things</w>', 'poet', 'gra', 'itt', 'ain', 'thing', 's--', 'mu', 'red</w>', 'e!</w>', 'happ', 'ened</w>', 'ke</w>', 'pl', 'ck</w>', 'It</w>', 'then</w>', 'me.</w>', 'king</w>', 'bac', 'find</w>', 'spir', 'ive</w>', 'cons', '’</w>', 'nec', 'myself</w>', 'art</w>', 'hear', 'small</w>', 'ligh', 'righ', 'ved</w>', 'teach', 'thy', 'mou', 'Zarathustra.</w>', 'ome</w>', 'sol', 'mountain', 'sun', 'etern', 'ye</w>', 'brethr', 'ther,</w>', 'many</w>', 'bod', 'ust</w>', 'ning</w>', 'dest', 'him,</w>', 'p</w>', 'time</w>', 'great', 'something</w>', 'ber', 'ed,</w>', 'ep', 'ow</w>', 'worl', 'if</w>', 'you</w>', 'about</w>', 'little</w>', 'f</w>', 'nigh', 'sle', 'ep</w>', 'dist', 'high', 'ese</w>', 'sure', 'beli', 'these</w>', 'sou', 'up</w>', 'sea', 'ked</w>', 'ound</w>', 'spo', 'me,</w>', 'lov', 'again</w>', 'happin', 'came</w>', ';', 'way', 'id</w>', 'ga', 'With</w>', 'ick', 'att', 'tor', 'As</w>', 'mat', 'heart', 'it.</w>', 's;</w>', 'speak</w>', 'dr', 'ile</w>', 'go</w>', 'oward', 'beh', 'bad</w>', 'Q', 'om</w>', 'her', 'ountain', 'speak', 'word', 'tal', 'rem', 'too</w>', 'ned</w>', 'ir</w>', 'jo', 'here</w>', 'soo', 'foo', 'pp', 'Now</w>', 'ither</w>', 'cr', 'them</w>', 'e-', 'som', 'If</w>', 'pi', 'ty</w>', 'see', 'es:</w>', 'ly', 'There</w>', 'thr', 'sha', 'col', 'wi', 'ything</w>', 'ily,</w>', 'fre', 'uten', 'itself</w>', 'est,</w>', 'unto</w>', 'thee,</w>', 'want</w>', 'rid', 'upon</w>', 'back', 'thou</w>', 'th.</w>', 'thee</w>', 'wanteth</w>', 'become</w>', 'learn</w>', 'MY</w>', 'mine</w>', 'ish', 'sc', 'let</w>', 'soul', 'danc', 'laugh', '4', 'lic', 'without</w>', 'themselves</w>', 'ready</w>', 'lec', 'ame</w>', 'see</w>', 'good</w>', 'self,</w>', 'answ', 'virtu', 'To</w>', 'When</w>', 'wear', 'heart</w>', 'da', 'spake</w>', 'thus</w>', 'Thou</w>', 'What</w>', 'dst</w>', 'hast</w>', 'oul', 'eag', 'awa', 'thine</w>', 'wis', 'dom', 'ly,</w>', 'ey', 'can', 'y!</w>', 'old', 's!</w>', 'down', 'ho', 'Then</w>', 'wilt</w>', 's?</w>', 'keth</w>', '?”</w>', 'answer', 'love</w>', 'said</w>', 'God', 'men,</w>', 'ove</w>', 'love', '!”</w>', 'es!</w>', 'They</w>', 'ust', 'believ', 'animal', 'make</w>', 'God</w>', 'Let</w>', 'themselv', 'Ye</w>', 'your</w>', 'tea', 'earth', 'you,</w>', 'brethren,</w>', 'ones</w>', 'y:</w>', 'against</w>', 'my', 'sin', 'wish', 'Oh,</w>', 'doth</w>', 'Verily,</w>', 'cometh</w>', 'just', 'heav', 'fr', 'anim', 'ers,</w>', 'bu', 'virtue</w>', 'aketh</w>', 'h,</w>', 'eth', 'ones,</w>', 'ones', 'teth</w>', '.--</w>', 'sil', 'Or</w>', 'gr', 'discour', 'hun', 'vo', 'self</w>', 'call', 'became</w>', 'ways</w>', 'own', 'is,</w>', 'how', 'bre', 'night', 'still', 'evil', 'ing!</w>', 'sw', 'me!</w>', 'leth</w>', 'cause</w>', 'wil', 'right</w>', 'pop', 'best</w>', 'verily,</w>', 'Ah,</w>', 'childr', 'yourselv', '_', 'fo', 'sel', 'wom', 'The', 'ough</w>', 'iet', '--</w>', 'ount</w>', 'ourselv', 'haps</w>', 'lones', 'NO', 'howev', 'litt', 'e’s</w>', ':', 'Nietzsche’s</w>', 'Nietzsch', 'Chapter</w>', 'Chap', 'ra</w>', 'nd</w>', 'Gutenberg', '/', 'Gutenberg-tm</w>', 'Gutenberg-t', '%', '@', '$'])\n",
            "Number of tokens: 1078\n",
            "==========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHz69mDY7f78",
        "colab_type": "text"
      },
      "source": [
        "Answer the following questions:\n",
        "\n",
        "**Study the subwords from your data. Do you see any subwords that make sense from the liguistic point of view? (e.g. suffixes, prefixes, common roots etc.). Provide examples.**\n",
        "\n",
        "<font color='green'>\"Zarathustra\" and other names make sense to be stand alone as they are specific names and repeat over and over in the text. </font>\n",
        "\n",
        "<font color='green'>The 'ay\\</w\\>' and alike make sense to be among new tokens as it pops up in such words as \"may\", \"way\", \"Gay\", \"bay\". However, such token is hard to comprehend at first sight - part of which word it is.</font>\n",
        "\n",
        "<font color='green'>There are stand alone specific words like \"with\", \"most\", \"Thus\" which make sense as those are words repeating in the language frequently.</font>\n",
        "\n",
        "<font color='green'>Interesting that it has created multiple tokens like 'howev', 'however,\\</w\\>', 'How\\</w\\>', 'how', which appear mostly in words like \"how\" and \"however\". But as they are presented in different variations (for example, with comma, capitalized first letter), then those would require their own tokens.</font>\n",
        "\n",
        "**What will happen if you increase the number of merges?**\n",
        "\n",
        "<font color='green'>I increased the number of merges to 3000 and then to 5000. I noticed that every time the number of tokens increased almost 3 and 5 times. However, I noticed that with increase in merges, there are more tokens looking like a full word rather than a set of letters. I assume that at some point the number of tokens would stop increasing.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8EqAI837f79",
        "colab_type": "text"
      },
      "source": [
        "#### Task 3.2 (0.75 points)\n",
        "\n",
        "Now, you are going to implement the function that splits the an unknown word into subwords using the vocab that we built above. \n",
        "\n",
        "One way to do it is the following:\n",
        "1. Sort our vocab by the length in the descending order.\n",
        "2. Find the boundaries of the \"window\" that is going to search if a candidate word has a corresponding subword in the vocab. In the beginning, the starting index is 0, since we start to scan the word from the first characher. The end index is the length of the longest subword in the vocab or the length of the word if it is smaller.\n",
        "3. In a while loop, start looking at the possible subwords. If the subword you are looking at is in the vocab, append it to the result. Now, your new starting index is your previous end index. Your new end index is your new start index plus the length of the longest subword in the vocab or the length of the word if it is smaller than the resulting sum. If the subword is not in the vocab, we reduce the end index by one thus narrowing our search window. Finally, is the length of our window is equal to one, we put an unknown subword in the result and update our window as above.\n",
        "4. End the loop when we reach the end of the word.\n",
        "\n",
        "After you finish with the function, test the tokenizer on a very common word and on a very unusual word (you can even try to invent a word yourself)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0e45a291-48e0-4bcb-b71a-370688dfc589",
        "id": "UgfBoNvauiKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Sorting the subwords by the length in the descending order\n",
        "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
        "\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    \"\"\"\n",
        "    Tokenizes the word into subword using learned BPE vocab\n",
        "    \n",
        "    Arguments:\n",
        "    string -- a word to tokenize. Must end with </w>\n",
        "    sorted_tokens -- sorted vocab by frequency in descending order\n",
        "    unknown_token -- a token to replace the words not found in the vocab\n",
        "    \"\"\"\n",
        "    \n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    # We are going to store our subwords here\n",
        "    string_tokens = []\n",
        "    \n",
        "    # Find the maximum length of the ngram in vocab\n",
        "    ngram_max_len = len(sorted_tokens[0])\n",
        "    # End index is the maximum lenth of the ngram or the length of the string is it's smaller\n",
        "    end_idx = (ngram_max_len if len(string)<ngram_max_len else len(string))\n",
        "    # Starting index is 0 in the beginning\n",
        "    start_idx = 0\n",
        "    \n",
        "    while start_idx < len(string):\n",
        "        subword = string[start_idx:end_idx]\n",
        "        if subword in sorted_tokens:\n",
        "            string_tokens.append(subword)\n",
        "            start_idx = end_idx\n",
        "            end_idx = start_idx + end_idx\n",
        "        elif len(subword) == 1:\n",
        "            string_tokens.append([unknown_token])\n",
        "            start_idx = end_idx\n",
        "            end_idx = start_idx + end_idx\n",
        "        else:\n",
        "            end_idx-=1\n",
        "            \n",
        "    return string_tokens\n",
        "\n",
        "# The word should end with \"</w>\". For example, \"cat</w>\".\n",
        "word_known = 'God</w>'\n",
        "word_unknown = 'Serendipity^~</w>'\n",
        "\n",
        "print('Tokenizing word: {}...'.format(word_known))\n",
        "if word_known in vocab_tokenization:\n",
        "    print(vocab_tokenization[word_known])\n",
        "else:\n",
        "    print(tokenize_word(string=word_known, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
        "    \n",
        "\n",
        "print('Tokenizing word: {}...'.format(word_unknown))\n",
        "if word_unknown in vocab_tokenization:\n",
        "    print(vocab_tokenization[word_unknown])\n",
        "else:\n",
        "    print(tokenize_word(string=word_unknown, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing word: God</w>...\n",
            "['God</w>']\n",
            "Tokenizing word: Serendipity^~</w>...\n",
            "['S', 'e', 're', 'n', 'di', 'pi', 'ty', ['</u>'], ['</u>'], '</w>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvfT_pL77f8F",
        "colab_type": "text"
      },
      "source": [
        "### Task 4. Lemmatization and normalization (1 point)\n",
        "\n",
        "#### Task 4.1 (0.5 points)\n",
        "\n",
        "Using either NTLK or Spacy, lemmatize your data.\n",
        "Make a copy of your data but this time transform all the tokens and lemmas into the lowercase.\n",
        "\n",
        "Provide the following statistics:\n",
        "- Number of unique lemmas (original case)\n",
        "- Number of unique lemmas (lower case)\n",
        "- Number of unique tokens (original case)\n",
        "- Number of unique tokens (lower case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6N7_kZc7f8L",
        "colab_type": "code",
        "outputId": "7a3250d0-851a-4768-e98b-326549996c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Lemmatize your data\n",
        "lemmas = ([token.lemma_ for token in doc])\n",
        "\n",
        "\n",
        "# Make a copy of your tokens but in lowercase\n",
        "lemmas_lower = ([token.lemma_.lower for token in doc])\n",
        "\n",
        "\n",
        "# Count statistics (no need to calculate the number of unique tokens in original case since we did it in Task 2)\n",
        "num_unique_lemmas = len(set(lemmas))\n",
        "num_unique_lemmas_lower = len(set(lemmas_lower))\n",
        "num_unique_tokens_lower = len(set([token.text.lower for token in doc]))\n",
        "\n",
        "# Print out the numbers\n",
        "print(\"Number of unique lemmas (original case):\", num_unique_lemmas)\n",
        "print(\"Number of unique lemmas (lower case):\", num_unique_lemmas_lower)\n",
        "print(\"Number of unique tokens (original case):\", num_unique_tokens)\n",
        "print(\"Number of unique tokens (lower case):\", num_unique_tokens_lower)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique lemmas (original case): 9668\n",
            "Number of unique lemmas (lower case): 120571\n",
            "Number of unique tokens (original case): 12209\n",
            "Number of unique tokens (lower case): 120173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8TsicOf7f8R",
        "colab_type": "text"
      },
      "source": [
        "#### Task 4.2 (0.5 points)\n",
        "\n",
        "Look at the numbers you got. \n",
        "\n",
        "**Imagine that you want to use your data to train a network that captures the meaning of the words. Do you want to use tokens or lemmas? Original or lowercase? Explain your choice.**\n",
        "\n",
        "<font color='green'>If we lemmatize - we get the base form of the word. We lose prefixes, suffixes which make morphological meaning of the word. For example if we remove \"al\" from \"critical\", we are left with \"critic\" which has a different meaning. Same applies to lowercase.</font>\n",
        "\n",
        "**Imagine that you want to use your data to train a system that detects named entities, i.e. names of people, places, companies etc. Do you want to use tokens or lemmas? Original or lowercase? Explain your choice.**\n",
        "\n",
        "<font color='green'>For ner I would prefer to have original casing (not lowercase) as it would be harder to detect NE right in lowercase English text. Capital letters can be a great hint. That is the reason why Chinese language can be hard for NE.\n",
        "Regarding Lemmatization - I would use lemmas, as for example token can be \"Jason's\", but we are interested in name \"Jason\".</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0i1kHsr7f8U",
        "colab_type": "text"
      },
      "source": [
        "### Task 5. Choose your pipeline (0.5 points)\n",
        "\n",
        "Choose the pipeline between [Spacy](https://spacy.io/) and [StanfordNLP](https://github.com/stanfordnlp/stanfordnlp).\n",
        "\n",
        "**Which pipeline did you choose? Why?**\n",
        "\n",
        "<font color='green'>StanfordNLP. Based on [this page](https://spacy.io/usage/facts-figures), comparison provided by Spacy made me note that StanfordNLP seems to be more advanced.</font>\n",
        "\n",
        "**What components does the pipeline have?**\n",
        "\n",
        "<font color='green'>“tokenizer, mwt, part-of-speech, lemmatization, dependency parsing”. Based on [this page](https://stanfordnlp.github.io/stanfordnlp/pipeline.html)</font>\n",
        "\n",
        "**What languages does the pipeline support?**\n",
        "\n",
        "<font color='green'>Stanfordnlp has more languages than SpaCy, as it is more \"mature\" and Spacy is \"younger\" and did not cover same langugages to same extend. Full list of languages can be found [here](https://stanfordnlp.github.io/stanfordnlp/models.html).</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUk5mzLA7f8V",
        "colab_type": "code",
        "outputId": "9105b799-1e4b-4000-edcd-dea9ce169013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "# import your pipeline here\n",
        "import stanfordnlp\n",
        "stanfordnlp.download('en')   \n",
        "nlp = stanfordnlp.Pipeline()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [02:08<00:00, 1.80MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcmWr6XE7f8Y",
        "colab_type": "text"
      },
      "source": [
        "### Task 6. Process your text (1.5 points)\n",
        "\n",
        "#### Task 6.1 (1 point)\n",
        "\n",
        "Process the text data from the first task with the pipeline of your choice. \n",
        "\n",
        "Select one sentence from the processed document and print out all the results (tokens, pos-tags, lemmas, depparse, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9EiDND57f8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "498e8aba-324f-4c1b-c385-f34ed4f564cf"
      },
      "source": [
        "# Process the text\n",
        "doc = nlp(data)\n",
        "\n",
        "\n",
        "# Print out the results\n",
        "doc.sentences[4].print_tokens()\n",
        "doc.sentences[4].print_dependencies() "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<Token index=1;words=[<Word index=1;text=Author;lemma=author;upos=NOUN;xpos=NN;feats=Number=Sing;governor=0;dependency_relation=root>]>\n",
            "<Token index=2;words=[<Word index=2;text=:;lemma=:;upos=PUNCT;xpos=:;feats=_;governor=1;dependency_relation=punct>]>\n",
            "<Token index=3;words=[<Word index=3;text=Friedrich;lemma=Friedrich;upos=PROPN;xpos=NNP;feats=Number=Sing;governor=1;dependency_relation=appos>]>\n",
            "<Token index=4;words=[<Word index=4;text=Nietzsche;lemma=Nietzsche;upos=PROPN;xpos=NNP;feats=Number=Sing;governor=3;dependency_relation=flat>]>\n",
            "('Author', '0', 'root')\n",
            "(':', '1', 'punct')\n",
            "('Friedrich', '1', 'appos')\n",
            "('Nietzsche', '3', 'flat')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPzJf19D7f8d",
        "colab_type": "text"
      },
      "source": [
        "#### Task 6.2 (0.5 points)\n",
        "\n",
        "**Look at your output above. Are the results correct? If no, provide the examples of the mistakes.**\n",
        "\n",
        "<font color='green'>In my case, specifically for 5th sentence in my text - it is all good. However if we check sentence 4, there is a word \"Book\", and lemma= \"Book\". IMO it is not correct.</font>\n",
        "\n",
        "**What is the difference between a POS tag and morphological tag?**\n",
        "\n",
        "<font color='green'>POS is about distinguishing which role the word plays in the sentence and based on it it is assigned to lexical category like noun, verb, adjective, adverb and other categories which depend on language. \n",
        "\n",
        "Morphological tagging aims to distinguish additional lexical and grammatical properties of words for morfologically rich languages like Turkish. The tags would distinguish Gender, Case etc.</font>\n",
        "\n",
        "**What is the difference between tagging and parsing?**\n",
        "\n",
        "<font color='green'>Tagging is about assigning a marker to each word in a text.\n",
        "Parsing (or dependency parsing) refers to building/showing lexical/syntatic dependencies between words. It can be found useful for free order languages.</font>\n",
        "\n",
        "**Analyze the dependency parsing result. Does it make sense? Briefly describe the meaning behind the relations.**\n",
        "\n",
        "<font color='green'>For sentence 5:\n",
        "Result makes sense.\n",
        "there is flat relation between Friedrich Nietzsche as it is a multiword expressions. Author is a root (head/governer) as it is the main gist of the sentence.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifQ06qSH7f8e",
        "colab_type": "text"
      },
      "source": [
        "### Task 7. Statistics (1 point)\n",
        "\n",
        "In your processed output, compute and print out (in a human readable format) the following stats:\n",
        "- POS tag frequency for each tag (in descending order)\n",
        "- 50 most frequent lemmas\n",
        "- 10 least frequent lemmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkxh0j4q7f8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "b118f61e-312c-4c45-97b9-079a45333353"
      },
      "source": [
        "print('POS tag frequency:')\n",
        "# Compute and print out POS tag frequency\n",
        "for words in (set([word.upos for sent in doc.sentences for word in sent.words])): \n",
        "  print(words, 'frequency is:', [word.upos for sent in doc.sentences for word in sent.words].count(words)) \n",
        "\n",
        "print('------\\n\\n50 most frequent lemmas:')\n",
        "# Compute and print out 50 most frequent lemmas\n",
        "print(Counter([word.lemma for sent in doc.sentences for word in sent.words]).most_common(50))\n",
        "\n",
        "print('------\\n\\n10 least frequent lemmas:')\n",
        "# Compute and print out 10 least frequent lemmas\n",
        "print(Counter([word.lemma for sent in doc.sentences for word in sent.words]).most_common()[:-11:-1])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POS tag frequency:\n",
            "PUNCT frequency is: 24451\n",
            "NOUN frequency is: 22057\n",
            "CCONJ frequency is: 5817\n",
            "VERB frequency is: 14187\n",
            "ADJ frequency is: 9041\n",
            "PART frequency is: 2711\n",
            "SCONJ frequency is: 1987\n",
            "INTJ frequency is: 740\n",
            "NUM frequency is: 795\n",
            "X frequency is: 172\n",
            "PROPN frequency is: 2473\n",
            "DET frequency is: 10697\n",
            "AUX frequency is: 7560\n",
            "SYM frequency is: 106\n",
            "PRON frequency is: 17156\n",
            "ADP frequency is: 12422\n",
            "ADV frequency is: 9218\n",
            "------\n",
            "\n",
            "50 most frequent lemmas:\n",
            "[(',', 8612), ('the', 6119), ('and', 4699), ('.', 4358), ('be', 4090), ('of', 3120), ('to', 2786), ('I', 2644), ('he', 2404), ('!', 2276), ('a', 2084), ('\"', 2082), (':', 1677), ('--', 1634), ('it', 1597), ('in', 1553), ('they', 1421), ('that', 1364), ('-', 1246), ('for', 1171), ('not', 1053), ('one', 1002), ('have', 996), ('my', 981), ('do', 979), ('all', 916), (';', 915), ('thou', 881), ('with', 875), ('?', 869), ('you', 731), ('but', 716), ('Zarathustra', 699), ('this', 682), ('ye', 659), ('man', 631), ('on', 564), ('we', 561), ('as', 547), ('who', 541), ('at', 517), ('there', 516), ('will', 493), ('which', 470), ('from', 454), ('what', 446), ('thus', 422), ('when', 395), ('hath', 387), ('thy', 384)]\n",
            "------\n",
            "\n",
            "10 least frequent lemmas:\n",
            "[('newsletter', 1), ('subscribe', 1), ('http://www.gutenberg.org', 1), ('facility', 1), ('Public', 1), ('network', 1), ('library', 1), ('originator', 1), ('professor', 1), ('http://pglaf.org/donate', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}