{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Copy of Homework1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rolnikolaygmailcom/NLP-tasks/blob/master/HW1/Homework1_NR_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1PLujMj7f7S",
        "colab_type": "text"
      },
      "source": [
        "## Homework 1\n",
        "### NLP Basics & NLP Pipelines\n",
        "\n",
        "Welcome to Homework 1! \n",
        "\n",
        "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _six_.\n",
        "\n",
        "The **grading** for each task is the following:\n",
        "- correct answer - **full points**\n",
        "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
        "- no answer or completely wrong solution - **no points**\n",
        "\n",
        "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
        "\n",
        "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
        "\n",
        "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
        "\n",
        "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
        "\n",
        "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64e7Qq2N7f7W",
        "colab_type": "code",
        "outputId": "4d9f588b-7910-4a26-f800-12a627ca4ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import spacy\n",
        "\n",
        "import tqdm\n",
        "import re\n",
        "from collections import defaultdict, Counter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5kMc2AD7f7c",
        "colab_type": "text"
      },
      "source": [
        "### Task 1. Find the data (0.5 points)\n",
        "\n",
        "Find large enough text data in English or [any other language supported by Spacy](https://spacy.io/usage/models). If the resources for your language are very limited, you may use English or other language of your preference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7qZ2wS_7f7h",
        "colab_type": "text"
      },
      "source": [
        "**What is the language of your data?**\n",
        "\n",
        "<font color='red'>English</font>\n",
        "\n",
        "**Where did you get the text data?**\n",
        "\n",
        "<font color='red'>Project Gutenberg</font>\n",
        "\n",
        "**What kind of text is it? (books, magazines, news articles, etc.)**\n",
        "\n",
        "<font color='red'>text from a book \"The Republic by Plato\"</font>\n",
        "\n",
        "**What style(s) of text does your data have? (user commetaries, scientific, neutral, etc.)**\n",
        "\n",
        "<font color='red'>Philosophy, Liturature</font>\n",
        "\n",
        "**Was it easy to download the data? If no, desribe what difficulties you had and how you resolved them.**\n",
        "\n",
        "<font color='red'>I wanted to have data to be as plain text, without url, references in parenthesis, or any extra text which might have caused issues. It is hard to find plain text. So I copy pasted text from the book, and created a new txt file.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_AQ8qQw7f7k",
        "colab_type": "text"
      },
      "source": [
        "### Task 2. Tokenize and count statistics (0.5 points)\n",
        "\n",
        "Using either NLTK or Spacy tools, tokenize your text data that you found in the previous exercise.\n",
        "\n",
        "P.S. if you are using Spacy, don't forget to load an appropriate module for it\n",
        "\n",
        "Compute and output the following:\n",
        "- number of sentences \n",
        "- number of tokens \n",
        "- number of unique tokens (or types)\n",
        "- average length of a sentence\n",
        "- average length of a token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILLRr9JV7f7o",
        "colab_type": "code",
        "outputId": "c2fa727b-4a64-4b6e-9034-5af0206118fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Replace the path with the name of your data file\n",
        "data_path = \"text.txt\"\n",
        "\n",
        "data = open(data_path, encoding='utf-8').read()\n",
        "data\n",
        "# Split the data into sentences and tokens\n",
        "print(word_tokenize(data))\n",
        "print(sent_tokenize(data))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['When', 'the', 'rhetorical', 'advantage', 'of', 'reciting', 'the', 'Dialogue', 'has', 'been', 'gained', ',', 'the', 'attention', 'is', 'not', 'distracted', 'by', 'any', 'reference', 'to', 'the', 'audience', ';', 'nor', 'is', 'the', 'reader', 'further', 'reminded', 'of', 'the', 'extraordinary', 'length', 'of', 'the', 'narrative', '.', 'Of', 'the', 'numerous', 'company', ',', 'three', 'only', 'take', 'any', 'serious', 'part', 'in', 'the', 'discussion', ';', 'nor', 'are', 'we', 'informed', 'whether', 'in', 'the', 'evening', 'they', 'went', 'to', 'the', 'torch-race', ',', 'or', 'talked', ',', 'as', 'in', 'the', 'Symposium', ',', 'through', 'the', 'night', '.', 'The', 'manner', 'in', 'which', 'the', 'conversation', 'has', 'arisen', 'is', 'described', 'as', 'follows', ':', '--', 'Socrates', 'and', 'his', 'companion', 'Glaucon', 'are', 'about', 'to', 'leave', 'the', 'festival', 'when', 'they', 'are', 'detained', 'by', 'a', 'message', 'from', 'Polemarchus', ',', 'who', 'speedily', 'appears', 'accompanied', 'by', 'Adeimantus', ',', 'the', 'brother', 'of', 'Glaucon', ',', 'and', 'with', 'playful', 'violence', 'compels', 'them', 'to', 'remain', ',', 'promising', 'them', 'not', 'only', 'the', 'torch-race', ',', 'but', 'the', 'pleasure', 'of', 'conversation', 'with', 'the', 'young', ',', 'which', 'to', 'Socrates', 'is', 'a', 'far', 'greater', 'attraction', '.', 'They', 'return', 'to', 'the', 'house', 'of', 'Cephalus', ',', 'Polemarchus', \"'\", 'father', ',', 'now', 'in', 'extreme', 'old', 'age', ',', 'who', 'is', 'found', 'sitting', 'upon', 'a', 'cushioned', 'seat', 'crowned', 'for', 'a', 'sacrifice', '.', \"'You\", 'should', 'come', 'to', 'me', 'oftener', ',', 'Socrates', ',', 'for', 'I', 'am', 'too', 'old', 'to', 'go', 'to', 'you', ';', 'and', 'at', 'my', 'time', 'of', 'life', ',', 'having', 'lost', 'other', 'pleasures', ',', 'I', 'care', 'the', 'more', 'for', 'conversation', '.', \"'Socrates\", 'asks', 'him', 'what', 'he', 'thinks', 'of', 'age', ',', 'to', 'which', 'the', 'old', 'man', 'replies', ',', 'that', 'the', 'sorrows', 'and', 'discontents', 'of', 'age', 'are', 'to', 'be', 'attributed', 'to', 'the', 'tempers', 'of', 'men', ',', 'and', 'that', 'age', 'is', 'a', 'time', 'of', 'peace', 'in', 'which', 'the', 'tyranny', 'of', 'the', 'passions', 'is', 'no', 'longer', 'felt', '.', 'Yes', ',', 'replies', 'Socrates', ',', 'but', 'the', 'world', 'will', 'say', ',', 'Cephalus', ',', 'that', 'you', 'are', 'happy', 'in', 'old', 'age', 'because', 'you', 'are', 'rich', '.', \"'And\", 'there', 'is', 'something', 'in', 'what', 'they', 'say', ',', 'Socrates', ',', 'but', 'not', 'so', 'much', 'as', 'they', 'imagine', '--', 'as', 'Themistocles', 'replied', 'to', 'the', 'Seriphian', ',', '``', 'Neither', 'you', ',', 'if', 'you', 'had', 'been', 'an', 'Athenian', ',', 'nor', 'I', ',', 'if', 'I', 'had', 'been', 'a', 'Seriphian', ',', 'would', 'ever', 'have', 'been', 'famous', ',', \"''\", 'I', 'might', 'in', 'like', 'manner', 'reply', 'to', 'you', ',', 'Neither', 'a', 'good', 'poor', 'man', 'can', 'be', 'happy', 'in', 'age', ',', 'nor', 'yet', 'a', 'bad', 'rich', 'man', '.', \"'\", 'Socrates', 'remarks', 'that', 'Cephalus', 'appears', 'not', 'to', 'care', 'about', 'riches', ',', 'a', 'quality', 'which', 'he', 'ascribes', 'to', 'his', 'having', 'inherited', ',', 'not', 'acquired', 'them', ',', 'and', 'would', 'like', 'to', 'know', 'what', 'he', 'considers', 'to', 'be', 'the', 'chief', 'advantage', 'of', 'them', '.', 'Cephalus', 'answers', 'that', 'when', 'you', 'are', 'old', 'the', 'belief', 'in', 'the', 'world', 'below', 'grows', 'upon', 'you', ',', 'and', 'then', 'to', 'have', 'done', 'justice', 'and', 'never', 'to', 'have', 'been', 'compelled', 'to', 'do', 'injustice', 'through', 'poverty', ',', 'and', 'never', 'to', 'have', 'deceived', 'anyone', ',', 'are', 'felt', 'to', 'be', 'unspeakable', 'blessings', '.', 'Socrates', ',', 'who', 'is', 'evidently', 'preparing', 'for', 'an', 'argument', ',', 'next', 'asks', ',', 'What', 'is', 'the', 'meaning', 'of', 'the', 'word', 'justice', '?', 'To', 'tell', 'the', 'truth', 'and', 'pay', 'your', 'debts', '?', 'No', 'more', 'than', 'this', '?', 'Or', 'must', 'we', 'admit', 'exceptions', '?', 'Ought', 'I', ',', 'for', 'example', ',', 'to', 'put', 'back', 'into', 'the', 'hands', 'of', 'my', 'friend', ',', 'who', 'has', 'gone', 'mad', ',', 'the', 'sword', 'which', 'I', 'borrowed', 'of', 'him', 'when', 'he', 'was', 'in', 'his', 'right', 'mind', '?', \"'There\", 'must', 'be', 'exceptions', '.', \"'\", \"'And\", 'yet', ',', \"'\", 'says', 'Polemarchus', ',', \"'the\", 'definition', 'which', 'has', 'been', 'given', 'has', 'the', 'authority', 'of', 'Simonides', '.', \"'\", 'Here', 'Cephalus', 'retires', 'to', 'look', 'after', 'the', 'sacrifices', ',', 'and', 'bequeaths', ',', 'as', 'Socrates', 'facetiously', 'remarks', ',', 'the', 'possession', 'of', 'the', 'argument', 'to', 'his', 'heir', ',', 'Polemarchus', '...']\n",
            "['When the rhetorical advantage of reciting the Dialogue has been gained, the attention is not distracted by any reference to the audience; nor is the reader further reminded of the extraordinary length of the narrative.', 'Of the numerous company, three only take any serious part in the discussion; nor are we informed whether in the evening they went to the torch-race, or talked, as in the Symposium, through the night.', 'The manner in which the conversation has arisen is described as follows:--Socrates and his companion Glaucon are about to leave the festival when they are detained by a message from Polemarchus, who speedily appears accompanied by Adeimantus, the brother of Glaucon, and with playful violence compels them to remain, promising them not only the torch-race, but the pleasure of conversation with the young, which to Socrates is a far greater attraction.', \"They return to the house of Cephalus, Polemarchus' father, now in extreme old age, who is found sitting upon a cushioned seat crowned for a sacrifice.\", \"'You should come to me oftener, Socrates, for I am too old to go to you; and at my time of life, having lost other pleasures, I care the more for conversation.\", \"'Socrates asks him what he thinks of age, to which the old man replies, that the sorrows and discontents of age are to be attributed to the tempers of men, and that age is a time of peace in which the tyranny of the passions is no longer felt.\", 'Yes, replies Socrates, but the world will say, Cephalus, that you are happy in old age because you are rich.', '\\'And there is something in what they say, Socrates, but not so much as they imagine--as Themistocles replied to the Seriphian, \"Neither you, if you had been an Athenian, nor I, if I had been a Seriphian, would ever have been famous,\" I might in like manner reply to you, Neither a good poor man can be happy in age, nor yet a bad rich man.\\'', 'Socrates remarks that Cephalus appears not to care about riches, a quality which he ascribes to his having inherited, not acquired them, and would like to know what he considers to be the chief advantage of them.', 'Cephalus answers that when you are old the belief in the world below grows upon you, and then to have done justice and never to have been compelled to do injustice through poverty, and never to have deceived anyone, are felt to be unspeakable blessings.', 'Socrates, who is evidently preparing for an argument, next asks, What is the meaning of the word justice?', 'To tell the truth and pay your debts?', 'No more than this?', 'Or must we admit exceptions?', 'Ought I, for example, to put back into the hands of my friend, who has gone mad, the sword which I borrowed of him when he was in his right mind?', \"'There must be exceptions.'\", \"'And yet,' says Polemarchus, 'the definition which has been given has the authority of Simonides.'\", 'Here Cephalus retires to look after the sacrifices, and bequeaths, as Socrates facetiously remarks, the possession of the argument to his heir, Polemarchus...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpZOv5gU7f7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_sentences = ...\n",
        "num_tokens = ...\n",
        "num_unique_tokens = ...\n",
        "avg_sentence_len = ...\n",
        "avg_token_len = ...\n",
        "\n",
        "print(\"Number of sentences:\", num_sentences)\n",
        "print(\"Number of tokens:\", num_tokens)\n",
        "print(\"Number of unique tokens (or types):\", num_unique_tokens)\n",
        "print(\"Average sentence length:\", avg_sentence_len)\n",
        "print(\"Average token length:\", avg_token_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHX-qiuI7f7z",
        "colab_type": "text"
      },
      "source": [
        "### Task 3. Byte pair encoding (BPE) tokenization (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C_PmQX-7f71",
        "colab_type": "text"
      },
      "source": [
        "#### Task 3.1 (0.25 points)\n",
        "\n",
        "[Byte pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) is a simple algorithm of data compression. It looks for the most frequent pair of bytes in the data and replaces it with a new byte which is not seen in the data. \n",
        "\n",
        "Recently, this idea became [used in the tokenization](https://www.aclweb.org/anthology/P16-1162.pdf). Let's say that we want to train a network that captures the meaning of words. We can have in out data the following words: `low`, `lower`, `lowest`. If we tokenize the text in a simple way by splitting the words as a whole, the model will probably learn the relation between `low`, `lower`, `lowest`. Now, imagine that we get some new text that the model didn't see during training and it has the words `small`, `smaller`, `smallest` and in the training data we had only the word `small`. Since the model didn't see `smaller` and `smallest` during the training, it will most likely fail to capture the relation.\n",
        "\n",
        "One of the ways to solve this is BPE tokenization. It learns the most frequent sequences and can split an unknown word into **subwords**. In our case, it can split `smaller` into `['small', 'er']` since we had `small` in the training data and probably many other words ending with -er. Now. instead of one unknown word, the model have two known subwords from which it can take the information.\n",
        "\n",
        "The code below builds the subwords from the text data. For the purpose of time saving, we set the number of merges to 1000. \n",
        "\n",
        "Study the code below and answer the questions after it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sec0p3h97f73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab(filename):\n",
        "    \"\"\"Gets the text from a file and splits it with spaces.\"\"\"\n",
        "    \n",
        "    vocab = Counter()\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    \"\"\"Computes the frequencies for each pair of characters in the vocab.\"\"\"\n",
        "\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, in_vocab):\n",
        "    \"\"\"Merges the most frequent pair.\n",
        "\n",
        "    Arguments:\n",
        "    pair -- the most frequent word pair (tuple(str, str))\n",
        "    in_vocab -- vocabulary with frequencies (dict)\n",
        "    \"\"\"\n",
        "    \n",
        "    out_vocab = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in in_vocab:\n",
        "        out_word = p.sub(''.join(pair), word)\n",
        "        out_vocab[out_word] = in_vocab[word]\n",
        "    return out_vocab\n",
        "\n",
        "def get_tokens_from_vocab(vocab):\n",
        "    tokens_frequencies = Counter()\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "def measure_token_length(token):\n",
        "    if token[-4:] == '</w>':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)\n",
        "\n",
        "vocab = get_vocab(data_path)\n",
        "\n",
        "print('==========')\n",
        "print('Tokens Before BPE')\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "print('==========')\n",
        "\n",
        "num_merges = 1000\n",
        "for i in tqdm(range(num_merges)):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "\n",
        "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "print('==========')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHz69mDY7f78",
        "colab_type": "text"
      },
      "source": [
        "Answer the following questions:\n",
        "\n",
        "**Study the subwords from your data. Do you see any subwords that make sense from the liguistic point of view? (e.g. suffixes, prefixes, common roots etc.). Provide examples.**\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "**What will happen if you increase the number of merges?**\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8EqAI837f79",
        "colab_type": "text"
      },
      "source": [
        "#### Task 3.2 (0.75 points)\n",
        "\n",
        "Now, you are going to implement the function that splits the an unknown word into subwords using the vocab that we built above. \n",
        "\n",
        "One way to do it is the following:\n",
        "1. Sort our vocab by the length in the descending order.\n",
        "2. Find the boundaries of the \"window\" that is going to search if a candidate word has a corresponding subword in the vocab. In the beginning, the starting index is 0, since we start to scan the word from the first characher. The end index is the length of the longest subword in the vocab or the length of the word if it is smaller.\n",
        "3. In a while loop, start looking at the possible subwords. If the subword you are looking at is in the vocab, append it to the result. Now, your new starting index is your previous end index. Your new end index is your new start index plus the length of the longest subword in the vocab or the length of the word if it is smaller than the resulting sum. If the subword is not in the vocab, we reduce the end index by one thus narrowing our search window. Finally, is the length of our window is equal to one, we put an unknown subword in the result and update our window as above.\n",
        "4. End the loop when we reach the end of the word.\n",
        "\n",
        "After you finish with the function, test the tokenizer on a very common word and on a very unusual word (you can even try to invent a word yourself)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8urP0gf7f7_",
        "colab_type": "code",
        "outputId": "84427c29-5c3c-452d-c4a1-8d4021795fe4",
        "colab": {}
      },
      "source": [
        "# Sorting the subwords by the length in the descending order\n",
        "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
        "\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    \"\"\"\n",
        "    Tokenizes the word into subword using learned BPE vocab\n",
        "    \n",
        "    Arguments:\n",
        "    string -- a word to tokenize. Must end with </w>\n",
        "    sorted_tokens -- sorted vocab by frequency in descending order\n",
        "    unknown_token -- a token to replace the words not found in the vocab\n",
        "    \"\"\"\n",
        "    \n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    # We are going to store our subwords here\n",
        "    string_tokens = []\n",
        "    \n",
        "    # Find the maximum length of the ngram in vocab\n",
        "    ngram_max_len = ...\n",
        "    # End index is the maximum lenth of the ngram or the length of the string is it's smaller\n",
        "    end_idx = ...\n",
        "    # Starting index is 0 in the beginning\n",
        "    start_idx = 0\n",
        "    \n",
        "    while start_idx < len(string):\n",
        "        subword = string[start_idx:end_idx]\n",
        "        if subword in sorted_tokens:\n",
        "            ...\n",
        "        elif len(subword) == 1:\n",
        "            ...\n",
        "        else:\n",
        "            ...\n",
        "            \n",
        "    return string_tokens\n",
        "\n",
        "# The word should end with \"</w>\". For example, \"cat</w>\".\n",
        "word_known = '...</w>'\n",
        "word_unknown = '...</w>'\n",
        "\n",
        "print('Tokenizing word: {}...'.format(word_known))\n",
        "if word_known in vocab_tokenization:\n",
        "    print(vocab_tokenization[word_known])\n",
        "else:\n",
        "    print(tokenize_word(string=word_known, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
        "    \n",
        "\n",
        "print('Tokenizing word: {}...'.format(word_unknown))\n",
        "if word_unknown in vocab_tokenization:\n",
        "    print(vocab_tokenization[word_unknown])\n",
        "else:\n",
        "    print(tokenize_word(string=word_unknown, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing word: mountains</w>...\n",
            "['mountain', 's</w>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvfT_pL77f8F",
        "colab_type": "text"
      },
      "source": [
        "### Task 4. Lemmatization and normalization (1 point)\n",
        "\n",
        "#### Task 4.1 (0.5 points)\n",
        "\n",
        "Using either NTLK or Spacy, lemmatize your data.\n",
        "Make a copy of your data but this time transform all the tokens and lemmas into the lowercase.\n",
        "\n",
        "Provide the following statistics:\n",
        "- Number of unique lemmas (original case)\n",
        "- Number of unique lemmas (lower case)\n",
        "- Number of unique tokens (original case)\n",
        "- Number of unique tokens (lower case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6N7_kZc7f8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatize your data\n",
        "...\n",
        "\n",
        "\n",
        "# Make a copy of your tokens but in lowercase\n",
        "...\n",
        "\n",
        "\n",
        "# Count statistics (no need to calculate the number of unique tokens in original case since we did it in Task 2)\n",
        "num_unique_lemmas = ...\n",
        "num_unique_lemmas_lower = ...\n",
        "num_unique_tokens_lower = ...\n",
        "\n",
        "# Print out the numbers\n",
        "print(\"Number of unique lemmas (original case):\", num_unique_lemmas)\n",
        "print(\"Number of unique lemmas (lower case):\", num_unique_lemmas_lower)\n",
        "print(\"Number of unique tokens (original case):\", num_unique_tokens)\n",
        "print(\"Number of unique tokens (lower case):\", num_unique_tokens_lower)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8TsicOf7f8R",
        "colab_type": "text"
      },
      "source": [
        "#### Task 4.2 (0.5 points)\n",
        "\n",
        "Look at the numbers you got. \n",
        "\n",
        "**Imagine that you want to use your data to train a network that captures the meaning of the words. Do you want to use tokens or lemmas? Original or lowercase? Explain your choice.**\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "**Imagine that you want to use your data to train a system that detects named entities, i.e. names of people, places, companies etc. Do you want to use tokens or lemmas? Original or lowercase? Explain your choice.**\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0i1kHsr7f8U",
        "colab_type": "text"
      },
      "source": [
        "### Task 5. Choose your pipeline (0.5 points)\n",
        "\n",
        "Choose the pipeline between [Spacy](https://spacy.io/) and [StanfordNLP](https://github.com/stanfordnlp/stanfordnlp).\n",
        "\n",
        "**Which pipeline did you choose? Why?**\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "**What components does the pipeline have?**\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "**What languages does the pipeline support?**\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUk5mzLA7f8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import your pipeline here\n",
        "import ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcmWr6XE7f8Y",
        "colab_type": "text"
      },
      "source": [
        "### Task 6. Process your text (1.5 points)\n",
        "\n",
        "#### Task 6.1 (1 point)\n",
        "\n",
        "Process the text data from the first task with the pipeline of your choice. \n",
        "\n",
        "Select one sentence from the processed document and print out all the results (tokens, pos-tags, lemmas, depparse, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9EiDND57f8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process the text\n",
        "...\n",
        "\n",
        "\n",
        "# Print out the results\n",
        "..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPzJf19D7f8d",
        "colab_type": "text"
      },
      "source": [
        "#### Task 6.2 (0.5 points)\n",
        "\n",
        "**Look at your output above. Are the results correct? If no, provide the examples of the mistakes.**\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "**What is the difference between a POS tag and morphological tag?**\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "**What is the difference between tagging and parsing?**\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "**Analyze the dependency parsing result. Does it make sense? Briefly describe the meaning behind the relations.**\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifQ06qSH7f8e",
        "colab_type": "text"
      },
      "source": [
        "### Task 7. Statistics (1 point)\n",
        "\n",
        "In your processed output, compute and print out (in a human readable format) the following stats:\n",
        "- POS tag frequency for each tag (in descending order)\n",
        "- 50 most frequent lemmas\n",
        "- 10 least frequent lemmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkxh0j4q7f8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute and print out POS tag frequency\n",
        "...\n",
        "\n",
        "\n",
        "# Compute and print out 50 most frequent lemmas\n",
        "...\n",
        "\n",
        "\n",
        "# Compute and print out 10 least frequent lemmas\n",
        "..."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}